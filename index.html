<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0041)https://people.eecs.berkeley.edu/~barron/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    heading2 {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 18px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 42px;
    }
    li:not(:last-child) {
        margin-bottom: 5px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <title>Akhilesh Gotmare</title>

  <link href="./_files/css" rel="stylesheet" type="text/css">
  <style id="dark-reader-style" type="text/css">@media screen {
/* Leading rule */
html {
  -webkit-filter: brightness(110%) contrast(90%) grayscale(20%) sepia(10%) !important;
}
/* Text contrast */
html {
  text-shadow: 0 0 0 !important;
}
/* Full screen */
*:-webkit-full-screen, *:-webkit-full-screen * {
  -webkit-filter: none !important;
}
/* Page background */
html {
  background: rgb(255,255,255) !important;
}
}</style></head>
  <body><div id="StayFocusd-infobar" style="display: none; top: 2400px;">
    <img src="chrome-extension://laankejkbhbdhmipfmgcngdelahlfoji/common/img/eye_19x19_red.png">
    <span id="StayFocusd-infobar-msg"></span>
    <span id="StayFocusd-infobar-links">
        <a id="StayFocusd-infobar-never-show">hide forever</a>&nbsp;&nbsp;|&nbsp;&nbsp;
        <a id="StayFocusd-infobar-hide">hide once</a>
    </span>
</div>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody><tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td width="67%" valign="middle">
        <p align="center">
        <name>Akhilesh Gotmare</name><br>
        dg.akhilesh at gmail dot com
        </p>
        <p>I am a Master's student at the Department of Computer Science at <a href="https://ic.epfl.ch/en">EPFL</a>. As 
          a part of my Master's program, I am currently interning with <a href="https://www.socher.org/"> 
          Dr. Richard Socher</a>'s Metamind team at Salesforce in
          Palo Alto. Earlier I worked as a Research Scholar student with 
          <a href = "https://mlo.epfl.ch">Prof. Martin Jaggi</a>'s 
          Machine Learning and Optimization laboratory at EPFL.
        </p>
        <p>
          Prior to joining EPFL, 
          I completed my undergraduate studies in Electrical Engineering (with a CSE minor) from <a href="http://iitgn.ac.in">IIT Gandhinagar</a>.
        </p>
        <p align="center">
<a href="https://scholar.google.com/citations?user=2S-aFwIAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
<a href="https://www.linkedin.com/in/akhilesh-gotmare/"> LinkedIn </a> &nbsp;/&nbsp;
<a href="https://raw.githubusercontent.com/akhileshgotmare/akhileshgotmare.github.io/master/files/cv-epfl-jul2018_22.pdf"> </a> CV &nbsp;/&nbsp;
<a href="https://github.com/akhileshgotmare"> GitHub </a> 
        </p>
        </td>
        <td width=270 height=250 >
        <img src="./files/img.jpg" width=270 height=250 > 
        </td>
      </tr>
  </tbody></table>

<!--       <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"> -->
<!--         <tbody><tr><td>
            <heading>News</heading>
            <ul>
              <li> At <a href="https://2018.icml.cc">ICML 2018</a>, I organized a workshop on efficient credit assignment. <a href="https://sites.google.com/view/creditassignmentindlanddrl/home">Efficient Credit Assignment in Deep Learning and Deep Reinforcement Learning </a>.</li>
              <li> At <a href="https://2017.icml.cc">ICML 2017</a>, I organized a workshop <a href="https://sites.google.com/view/icml-reproducibility-workshop/home">Workshop on Reproducibility in Machine Learning</a>.</li>
            </ul>
        </td></tr>
 -->
<!--         <tr><td>
            <heading>Invited Talks and Lectures</heading>
            <ul>
              <li> In April 2018, I gave an invited talk at <a href="http://dalimeeting.org/dali2018//program"</a> DALI'2018 </li>
              <li> In April 2018, I gave an invited talk at <a href="https://www.ucl.ac.uk/"</a> UCL Machine learning Department </li>
              <li> In July 2016, I gave a talk on my work Professor Forcing: A new way of training RNN's<a href="https://papers.nips.cc/paper/6099-professor-forcing-a-new-algorithm-for-training-recurrent-networks</a> (slides <a href="files/professor_forcing.pdf"> here </a> ). </li>
            </ul>
        </td></tr> -->
<!--       <tr>
        <td width="100%" valign="middle">
          <heading>Publications</heading>
          <p>
          These days, most of my time goes in thinking how my brains do efficient credit assignment in time ? 
          For RL problems, I'd like to figure out a method that estimates the gradient of the reward with respect
          to the action probabilities in a way that mimics some of the fundamental properties of
          backprop. Backprop works by *composing* local estimates of effects (Jacobians).
          In general I'm interested in generative models, overcoming catestrophoc forgetting in neural networks, 
          credit assignment in discrete action space.  
          </p>
        </td>
      </tr>
     </tbody></table> -->
       <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Publications</heading>
        </td>
      </tr>
      </table>
     <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tr>
        <td width="25%">
          <img src='iros2018.png'>
        </td>
        <td valign="top" width="75%">
          <p>
              <papertitle>Using Mode Connectivity for Loss Landscape Analysis</papertitle>
            <br>
            <strong>Akhilesh Gotmare</strong>, Nitish Shirish Keskar, Caiming Xiong and Richard Socher
            <br>
            <em> ICML 2018 Workshop on Modern Trends in Nonconvex Optimization for Machine Learning</em>, July, Stockholm (Swede).
            <br>
          <a href="https://arxiv.org/abs/1806.06977"> arXiv</a>
<!--             <a href="https://www.nada.kth.se/cas/data/handtool/"> dataset</a> -->
<!--             <a href="https://raw.githubusercontent.com/mancinimassimiliano/mancinimassimiliano.github.io/master/iros2018.bib"> bibtex</a></p> -->
        </td>
      </tr>
       
        <tr onmouseout="place_stop()" onmouseover="place_start()">
          <td width="25%">
            <heading>Publications</heading>
            <script type="text/javascript">
            function place_start() {
              document.getElementById('place_gif').style.opacity = "1";
            }
   function place_stop() {
              document.getElementById('place_gif').style.opacity = "0";
            }
            place_stop()
            </script>
              </td>
              <td valign="top" width="75%">
                <heading2><i></i></heading2><br>
              <p><a href="">
                <papertitle> Using Mode Connectivity for Loss Landscape Analysis</papertitle></a><br>
              <strong>Akhilesh Gotmare</strong>, 
              Nitish Shirish Keskar,
              Caiming Xiong,
              Richard Socher,
                <a href="https://sites.google.com/view/icml2018nonconvex/papers">ICML 2018 Workshop on Modern Trends 
                  in Nonconvex Optimization for Machine Learning</a>
              </p><p></p>
              <p>
                Mode connectivity is a recently introduced frame-
                work that empirically establishes the connected-
                ness of minima by finding a high accuracy curve

                between two independently trained models. To
                investigate the limits of this setup, we examine
                the efficacy of this technique in extreme cases
                where the input models are trained or initialized
                differently. We find that the procedure is resilient
                to such changes. Given this finding, we propose
                using the framework for analyzing loss surfaces
                and training trajectories more generally, and in
                this direction, study SGD with cosine annealing
                and restarts (SGDR). We report that while SGDR
                moves over barriers in its trajectory, propositions
                claiming that it converges to and escapes from
                multiple local minima are not substantiated by
                our empirical results.
                </p>
              </td>
            </tr>
        <tr onmouseout="place_stop()" onmouseover="place_start()">
          <td width="25%">
            <div class="one">
                <div class="two" id="place_gif" style="opacity: 0;"><img src="./files/fortified_nets.png" width=150 height=150></div>
                <img src="./files/fortified_nets.png" width=150 height=150>
            </div>
            <script type="text/javascript">
            function place_start() {
              document.getElementById('place_gif').style.opacity = "1";
            }
   function place_stop() {
              document.getElementById('place_gif').style.opacity = "0";
            }
            place_stop()
            </script>
              </td>
              <td valign="top" width="75%">
                <heading2><i></i></heading2><br>
              <p><a href="">
                <papertitle>Decoupling Backpropagation using Constrained Optimization Methods</papertitle></a><br>
              <strong>Akhilesh Gotmare*</strong>,
              Valentin Thomas*, 
              Johanni Brea,
              Martin Jaggi,
                <a href="https://arxiv.org/abs/1804.02485">arXiv</a>
                /
                <a href="https://github.com/anirudh9119/fortified-networks">Code</a>
              </p><p></p>
              <p>
               Deep networks have achieved impressive results across a variety of important tasks. However a known weakness is a failure to perform well when evaluated on data which differ from the training distribution, even if these differences are very small, as is the case with adversarial examples. We propose Fortified Networks, a simple transformation of existing networks, which fortifies the hidden layers in a deep network by identifying when the hidden states are off of the
               data manifold, and maps these hidden states back to parts of the data manifold where the network performs well. Our principal contribution is to show that fortifying these hidden states improves the robustness of deep networks and our experiments (i) demonstrate improved robustness to standard adversarial attacks in both black-box and white-box threat models; (ii) suggest that our improvements are not primarily due to the gradient masking problem and (iii) show the
               advantage of doing this fortification in the hidden layers instead of the input space.
              </p>
              </td>
            </tr>
        <tr onmouseout="place_stop()" onmouseover="place_start()">
          <td width="25%">
            <div class="one">
                <div class="two" id="place_gif" style="opacity: 0;"><img src="./files/recall_traces.png" width=150 height=150></div>
                <img src="./files/recall_traces.png" width=150 height=150>
            </div>
            <script type="text/javascript">
            function place_start() {
              document.getElementById('place_gif').style.opacity = "1";
            }
   function place_stop() {
              document.getElementById('place_gif').style.opacity = "0";
            }
            place_stop()
            </script>
              </td>
              <td valign="top" width="75%">
                <heading2><i></i></heading2><br>
              <p><a href="">
                <papertitle>Recall Traces: Backtracking Models for Efficient Reinforcement Learning</papertitle></a><br>
              <strong>Anirudh Goyal*</strong>,
              Philemon Brakel,
              William Fedus,
              Timothy Lillicrap,
              Sergey Levine, 
              Hugo Larochelle,
              <a href="www.iro.umontreal.ca/~bengioy/yoshua_en/">Yoshua Bengio</a>,
                <a href="https://arxiv.org/abs/1804.00379">arXiv</a>
                /
                code (coming soon)
              </p><p></p>
              <p>
              In many environments only a tiny subset of all states yield high reward. In these cases, few of the interactions with the environment provide a relevant learning signal. Hence, we may want to preferentially train on those high-reward states and the probable trajectories leading to them. To this end, we advocate for the use of a backtracking model that predicts the preceding states that terminate at a given high-reward state. We can train a model which, starting from a high
              value state (or one that is estimated to have high value), predicts and sample for which the (state, action)-tuples may have led to that high value state. These traces of (state, action) pairs, which we refer to as Recall Traces, sampled from this backtracking model starting from a high value state, are informative as they terminate in good states, and hence we can use these traces to improve a policy. We provide a variational interpretation for this idea and a practical
              algorithm in which the backtracking model samples from an approximate posterior distribution over trajectories which lead to large rewards. Our method improves the sample efficiency of both on- and off-policy RL algorithms across several environments and tasks.
              </p>
              </td>
            </tr>
        <tr onmouseout="place_stop()" onmouseover="place_start()">
          <td width="25%">
            <div class="one">
                <div class="two" id="place_gif" style="opacity: 0;"><img src="./files/z_forcing" width=150 height=150></div>
                <img src="./files/z_forcing.png" width=150 height=150>
            </div>
            <script type="text/javascript">
            function place_start() {
              document.getElementById('place_gif').style.opacity = "1";
            }
   function place_stop() {
              document.getElementById('place_gif').style.opacity = "0";
            }
            place_stop()
            </script>
              </td>
              <td valign="top" width="75%">
                <heading2><i></i></heading2><br>
              <p><a href="">
                <papertitle>Z Forcing: Training Stochastic RNN's</papertitle></a><br>
              <strong>Anirudh Goyal*</strong>,
              Alessandro Sordoni*,
              Marc-Alexandre Côté,
              Rosemary Nan Ke,
              <a href="www.iro.umontreal.ca/~bengioy/yoshua_en/">Yoshua Bengio</a>,
              <em>Neural Information Processing System (NIPS)</em>, 2017 <br>
                <a href="">arXiv</a>
                /
                <a href="https://github.com/anirudh9119/zforcing_nips17">code</a>
              </p><p></p>
              <p> 
              We proposed a novel approach to incorporate stochastic latent variables in sequential neural networks. The method builds on recent architectures that use latent variables to condition the recurrent dynamics of the network. We augmented the inference network with an RNN that runs backward through the sequence and added a new auxiliary cost that forces the latent variables to reconstruct the state of  that backward RNN, i.e. predict a summary of future observations.
              </p>
              </td>
            </tr>
        <tr onmouseout="maml_stop()" onmouseover="maml_start()">
          <td width="25%">
            <div class="one">
                <div class="two" id="maml_image" style="opacity: 0;"><img src="./files/vw.png" width=150 height=130></div>
                <img src="./files/vw.png" width=150 height=130>
            </div>
            <script type="text/javascript">
            function maml_start() {
              document.getElementById('maml_image').style.opacity = "1";
            }
   function maml_stop() {
              document.getElementById('maml_image').style.opacity = "0";
            }
            maml_stop()
            </script>
              </td>
              <td valign="top" width="75%">
              <p><a href="">
                <papertitle>Variational Walkback: Learning a Transition Operator as a Stochastic Recurrent Net</papertitle></a><br>
              <strong>Anirudh Goyal</strong>,
              Nan Rosemary Ke, 
              <a href="https://ganguli-gang.stanford.edu/surya.html">Surya Ganguli</a>,
              <a href="www.iro.umontreal.ca/~bengioy/yoshua_en/
              ">Yoshua Bengio</a> <br>
              <em>Neural Information Processing System (NIPS)</em>, 2017 <br>
                <a href="">arXiv</a>
                /
                blog post (coming soon)
                /
                code (coming soon)
              </p><p></p>
              <p> 
               We propose a novel method to directly learn a stochastic transition operator whose repeated application provides generated samples. Traditional undirected graphical models approach this problem indirectly by learning a Markov chain model whose stationary distribution obeys detailed balance with respect to a parameterized energy function.
              </p>
              </td>
            </tr>
      <tr onmouseout="sirfs_stop()" onmouseover="sirfs_start()">
          <td width="25%">
            <div class="one">
                <div class="two" id="sirfs_image" style="opacity: 0;"><img src="./files/sab.png" style="border-style: none" width="150"></div>
                <img src="./files/sab.png" style="border-style: none" width="150">
            </div>
            <script type="text/javascript">
            function sirfs_start() {
              document.getElementById('sirfs_image').style.opacity = "1";
            }
            function sirfs_stop() {
              document.getElementById('sirfs_image').style.opacity = "0";
            }
            sirfs_stop()
            </script>
          </td>
        <td width="75%" valign="top">
        <p>
          <a href="http://padl.ws/papers/Paper%2039.pdf" id="SIRFS">
          <papertitle>Sparse Attentive BackTracking: An Efficient way of doing credit assignment in RNN's</papertitle>
          </a>
          <br>
          Rosemary Nan Ke, <strong>Anirudh Goyal</strong>, Olexa Bilaniuk, Chris Pal, Yoshua Bengio <br>
          <em>ICML Workshop on Principled Approaches to Deep Learning</em>, 2017<br>
        </p>
        <p>
         We learns an attention mechanism
         over the hidden states in the past and selectively
         backpropagates through paths with high
         attention weight. This allows the model to learn
         long term dependencies while only backtracking
         for a small number of steps into the past. We think, its an efficient way of doing credit assignment in time.
        </p>
        </td>
      </tr>
        <tr onmouseout="ssrl_stop()" onmouseover="ssrl_start()">
          <td width="25%">
                  <heading2><i></i></heading2><br>
            <div class="one">
                <div class="two" id="ssrl_image" style="opacity: 0;"><img src="./files/zoneout.png" width=150 height=130></div>
                <img src="./files/zoneout.png" width=150 height=130>
            </div>
            <script type="text/javascript">
            function ssrl_start() {
              document.getElementById('ssrl_image').style.opacity = "1";
            }
            function ssrl_stop() {
              document.getElementById('ssrl_image').style.opacity = "0";
            }
            ssrl_stop()
            </script>
              </td>
              <td valign="top" width="75%">
              <p><a href="https://arxiv.org/abs/1606.01305">
                <papertitle>Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations</papertitle></a><br>
              David Krueger, Tegan Maharaj, Janos Kramar, Mohammad Pezeshki, Nicolas Ballas, Nan Rosemary Ke,<strong> Anirudh Goyal</strong>
              <a href="www.iro.umontreal.ca/~bengioy/yoshua_en/">Yoshua Bengio</a>,
              <a href="https://aaroncourville.wordpress.com/">Aaron Courville</a> <br>
              <a href="www.professeurs.polymtl.ca/christopher.pal/">Chris Pal</a> <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2017 <br> <!-- &nbsp; <font color="red"><strong>(Oral Presentation)</strong></font> <br-->
                <a href="https://arxiv.org/abs/1606.01305">arXiv</a>
                /
                <a href="https://github.com/teganmaharaj/zoneout">code</a>
              </p><p></p>
              <p>
              We propose zoneout, a novel method for regularizing RNNs. At each timestep, zoneout stochastically forces some hidden units to maintain their previous values. Like dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization. But by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks. 
              </p>
              </td>
            </tr>
        <tr onmouseout="mpc_stop()" onmouseover="mpc_start()">
          <td width="25%">
            <div class="one">
            </div>
            <script type="text/javascript">
            function mpc_start() {
              document.getElementById('mpc_image').style.opacity = "1";
            }
            function mpc_stop() {
              document.getElementById('mpc_image').style.opacity = "0";
            }
            mpc_stop()
            </script>
              </td>
              <td valign="top" width="75%">
              <p><a href="">
                <papertitle>ACtuAL: Actor-Critic Under Adversarial Learning</papertitle></a><br>
                <strong>Anirudh Goyal</strong>, Nan Rosemary Ke, Alex Lamb, R Devon Hjelm, Chris Pal, <a href="www.cs.mcgill.ca/~jpineau/">Joelle Pineau</a>,
              <a href="www.iro.umontreal.ca/~bengioy/yoshua_en/">Yoshua Bengio</a>
                <a href="https://arxiv.org/abs/1711.04755">arXiv</a>
                /
                <a href="https://github.com/rizar/actor-critic-public">code</a>
              </p><p></p>
              <p>
              Generative Adversarial Networks (GANs) are a powerful framework for deep generative modeling. Posed as a two-player minimax problem, GANs are typically trained end-to-end on real-valued data and can be used to train a generator of high-dimensional and realistic images. However, a major limitation of GANs is that training relies on passing gradients from the discriminator through the generator via back-propagation. This makes it fundamentally difficult to train GANs with
              discrete data, as generation in this case typically involves a non-differentiable function. These difficulties extend to the reinforcement learning setting when the action space is composed of discrete decisions. We address these issues by reframing the GAN framework so that the generator is no longer trained using gradients through the discriminator, but is instead trained using a learned critic in the actor-critic framework with a Temporal Difference (TD) objective. This
              is a natural fit for sequence modeling and we use it to achieve improvements on language modeling tasks over the standard Teacher-Forcing methods.
              </p>
              </td>
            </tr>
        <tr onmouseout="mpc_stop()" onmouseover="mpc_start()">
          <td width="25%">
            <div class="one">
                <div class="two" id="mpc_image" style="opacity: 0;"><img src="./files/actor_crticic.png" width=150 height=130></div>
                <img src="./files/actor_crticic.png" width=150 height=130>
            </div>
            <script type="text/javascript">
            function mpc_start() {
              document.getElementById('mpc_image').style.opacity = "1";
            }
            function mpc_stop() {
              document.getElementById('mpc_image').style.opacity = "0";
            }
            mpc_stop()
            </script>
              </td>
              <td valign="top" width="75%">
              <p><a href="">
                <papertitle>An Actor-Critic Algorithm for Sequence Prediction</papertitle></a><br>
                Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, <strong>Anirudh Goyal</strong>, Ryan Lowe, <a href="www.cs.mcgill.ca/~jpineau/">Joelle Pineau</a>,
              <a href="https://aaroncourville.wordpress.com/">Aaron Courville</a>,
              <a href="www.iro.umontreal.ca/~bengioy/yoshua_en/">Yoshua Bengio</a>
              <em>International Conference on Learning Representations (ICLR)</em>, 2017 <br> <!-- &nbsp; <font color="red"><strong>(Oral Presentation)</strong></font> <br-->
                <a href="https://arxiv.org/abs/1607.07086">arXiv</a>
                /
                <a href="https://github.com/rizar/actor-critic-public">code</a>
              </p><p></p>
              <p>
              We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a critic network that is trained to predict the value of an output token, given the policy of an actor network
              </p>
              </td>
            </tr>
        <tr onmouseout="rfgps_stop()" onmouseover="rfgps_start()">
          <td width="25%">
            <div class="one">
                <div class="two" id="rfgps_image" style="opacity: 0;"><img src="./files/pf_nips16.png" width=150 height=130></div>
                        <img src="./files/pf_nips16.png" width=150 height=130>
            </div>
            <script type="text/javascript">
            function rfgps_start() {
              document.getElementById('rfgps_image').style.opacity = "1";
            }
            function rfgps_stop() {
              document.getElementById('rfgps_image').style.opacity = "0";
            }
            rfgps_stop()
            </script>
              </td>
              <td valign="top" width="75%">
                <p><a href="https://papers.nips.cc/paper/6099-professor-forcing-a-new-algorithm-for-training-recurrent-networks.pdf">
                <papertitle>Professor Forcing: A New Algorithm for Training Recurrent Networks</papertitle></a><br>
              <strong>Anirudh Goyal</strong>, Alex Lamb, Ying Zhang, Saizheng Zhang, 
              <a href="https://aaroncourville.wordpress.com/">Aaron Courville</a>,
              <a href="www.iro.umontreal.ca/~bengioy/yoshua_en/">Yoshua Bengio</a>,
              <em> Neural Information on Processing System(NIPS)</em>, 2016 <br> <!-- &nbsp; <font color="red"><strong>(Oral Presentation)</strong></font> <br-->
                <a href="https://arxiv.org/abs/1610.09038">arXiv</a>
                /
                <a href="http://videolectures.net/deeplearning2016_goyal_new_algorithm/">video</a>
                /
                <a href="https://github.com/anirudh9119/LM_GANS">code</a>
              </p><p></p>
              <p>
              The Teacher Forcing algorithm trains recurrent networks by supplying observed sequence values as inputs during training and using the network’s own one-step-ahead predictions to do multi-step sampling. We introduce the Professor Forcing algorithm, which uses adversarial domain adaptation to encourage the dynamics of the recurrent network to be the same when training the network and when sampling from the network over multiple time steps.
              </p>
              </td>
            </tr>
      </tbody></table>
    </td>
    </tr>
  </tbody></table>
</body></html>
